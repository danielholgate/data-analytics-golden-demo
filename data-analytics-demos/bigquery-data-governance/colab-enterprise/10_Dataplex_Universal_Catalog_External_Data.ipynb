{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Wx2xZrPfN2"
      },
      "source": [
        "### <font color='#4285f4'>Dataplex Universal Catalog external data - Overview</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-6J2atTPfN2"
      },
      "source": [
        "Overview: This notebook import metadata from a database (Oracle) running outside of Google Cloud into Dataplex Universal Catalog. It provides an example of how data from external systems can be integrated into the catalog and made available for search.\n",
        "\n",
        "Approach:\n",
        "1. The Aspect Types, Entry Types, and Entry Group found in the metadata import file are first generated in the project via python APIs.\n",
        "2. A json import request file is downloaded from Cloud Storage to the notebook instance.\n",
        "3. The metadata import API is invoked via curl, passing in the import request file. The import process loads the metadata import file for the Oracle database from Cloud Storage into Dataplex Universal Catalog. Note that the import process is async and takes approximately 8 minutes to run.\n",
        "\n",
        "Author:\n",
        "* Daniel Holgate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN2wUQ1tPfN2"
      },
      "outputs": [],
      "source": [
        "# Architecture Diagram\n",
        "from IPython.display import Image\n",
        "Image(url='https://storage.googleapis.com/data-analytics-golden-demo/colab-diagrams/BigQuery-Data-Governance-Automated-Data-Governance.png', width=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-D4JI3QPfN3"
      },
      "source": [
        "### <font color='#4285f4'>Video Walkthrough</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twos9IjzPfN3"
      },
      "source": [
        "TBD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMsUvoF4BP7Y"
      },
      "source": [
        "### <font color='#4285f4'>License</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQgQkbOvj55d"
      },
      "source": [
        "```\n",
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m65vp54BUFRi"
      },
      "source": [
        "### <font color='#4285f4'>Pip installs</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MaWM6H5i6rX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# PIP Installs (if necessary)\n",
        "import sys\n",
        "\n",
        "!{sys.executable} -m pip install google-cloud-dataplex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmyL-Rg4Dr_f"
      },
      "source": [
        "### <font color='#4285f4'>Initialize</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOYsEVSXp6IP"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import HTML\n",
        "import IPython.display\n",
        "import google.auth\n",
        "import requests\n",
        "import json\n",
        "import uuid\n",
        "import base64\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import base64\n",
        "import random\n",
        "\n",
        "import re\n",
        "import argparse\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "from google.cloud import dataplex_v1\n",
        "\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "import requests\n",
        "\n",
        "import logging\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt, before_sleep_log, retry_if_exception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMlHl3bnkFPZ"
      },
      "outputs": [],
      "source": [
        "# Set these (run this cell to verify the output)\n",
        "dataplex_location = \"${dataplex_location}\"\n",
        "governed_data_code_bucket = \"${governed_data_code_bucket}\"\n",
        "\n",
        "# Get the current date and time\n",
        "now = datetime.datetime.now()\n",
        "\n",
        "# Format the date and time as desired\n",
        "formatted_date = now.strftime(\"%Y-%m-%d-%H-%M\")\n",
        "\n",
        "# Get some values using gcloud\n",
        "project_id = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "user = !(gcloud auth list --filter=status:ACTIVE --format=\"value(account)\")\n",
        "\n",
        "if len(user) != 1:\n",
        "  raise RuntimeError(f\"user is not set: {user}\")\n",
        "user = user[0]\n",
        "\n",
        "print(f\"project_id = {project_id}\")\n",
        "print(f\"user = {user}\")\n",
        "print(f\"dataplex_location = {dataplex_location}\")\n",
        "print(f\"governed_data_code_bucket = {governed_data_code_bucket}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ6m_wGrK0YG"
      },
      "source": [
        "### <font color='#4285f4'>Helper Methods</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbOjdSP1kN9T"
      },
      "source": [
        "#### restAPIHelper\n",
        "Calls the Google Cloud REST API using the current users credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40wlwnY4kM11"
      },
      "outputs": [],
      "source": [
        "def restAPIHelper(url: str, http_verb: str, request_body: str) -> str:\n",
        "  \"\"\"Calls the Google Cloud REST API passing in the current users credentials\"\"\"\n",
        "\n",
        "  import google.auth.transport.requests\n",
        "  import requests\n",
        "  import google.auth\n",
        "  import json\n",
        "\n",
        "  # Get an access token based upon the current user\n",
        "  creds, project = google.auth.default()\n",
        "  auth_req = google.auth.transport.requests.Request()\n",
        "  creds.refresh(auth_req)\n",
        "  access_token=creds.token\n",
        "\n",
        "  headers = {\n",
        "    \"Content-Type\" : \"application/json\",\n",
        "    \"Authorization\" : \"Bearer \" + access_token\n",
        "  }\n",
        "\n",
        "  if http_verb == \"GET\":\n",
        "    response = requests.get(url, headers=headers)\n",
        "  elif http_verb == \"POST\":\n",
        "    response = requests.post(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PUT\":\n",
        "    response = requests.put(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PATCH\":\n",
        "    response = requests.patch(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"DELETE\":\n",
        "    response = requests.delete(url, headers=headers)\n",
        "  else:\n",
        "    raise RuntimeError(f\"Unknown HTTP verb: {http_verb}\")\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    return json.loads(response.content)\n",
        "    #image_data = json.loads(response.content)[\"predictions\"][0][\"bytesBase64Encoded\"]\n",
        "  else:\n",
        "    error = f\"Error restAPIHelper -> ' Status: '{response.status_code}' Text: '{response.text}'\"\n",
        "    raise RuntimeError(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### File Helper"
      ],
      "metadata": {
        "id": "qsXK3lMTTWba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns content of a file\n",
        "def loadReferencedFile(file_path) -> str:\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"Error while reading file {file_path}: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "6r_x8UXPTZ9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c51M89g0Ejmz"
      },
      "source": [
        "### <font color='#4285f4'>Create Dataplex Universal Catalog metadata hierarchy</font>\n",
        "\n",
        "Create the appropriate Dataplex metadata hierarchy (entry types, aspect types, entry group) before loading the metadata import file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_entry_group(\n",
        "    project_id: str, location: str, entry_group_id: str\n",
        ") -> dataplex_v1.EntryGroup:\n",
        "    \"\"\"Create Entry Group with entry_group_id,in project_id and location \"\"\"\n",
        "\n",
        "    with dataplex_v1.CatalogServiceClient() as client:\n",
        "        # The resource name of the Entry Group location\n",
        "        parent = f\"projects/{project_id}/locations/{location}\"\n",
        "        entry_group = dataplex_v1.EntryGroup(\n",
        "            description=f\"Entry group {entry_group_id} description\"\n",
        "        )\n",
        "        create_operation = client.create_entry_group(\n",
        "            parent=parent, entry_group=entry_group, entry_group_id=entry_group_id\n",
        "        )\n",
        "        print(f\"Created Entry Group: projects/{project_id}/locations/{location}/entryGroups/{entry_group_id}\")\n",
        "        return create_operation.result(60)"
      ],
      "metadata": {
        "id": "fTL24X3vn4vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_entry_type(\n",
        "    project_id: str, location: str, entry_type_id: str\n",
        ") -> dataplex_v1.EntryType:\n",
        "    \"\"\"Create Entry Type identified by entry_type_id, located in project_id, location \"\"\"\n",
        "\n",
        "    print(f\"Creating Entry Type {entry_type_id}\")\n",
        "\n",
        "    typeAliases = []\n",
        "\n",
        "    # To improve search and discoverability, create some common aliases for the oracle records\n",
        "    if \"-table\" in entry_type_id:\n",
        "        typeAliases.append(\"TABLE\")\n",
        "\n",
        "    if \"-view\" in entry_type_id:\n",
        "        typeAliases.append(\"VIEW\")\n",
        "\n",
        "    if \"-database\" in entry_type_id:\n",
        "        typeAliases.append(\"DATABASE\")\n",
        "\n",
        "    with dataplex_v1.CatalogServiceClient() as client:\n",
        "        parent = f\"projects/{project_id}/locations/{location}\"\n",
        "        entry_type = dataplex_v1.EntryType(\n",
        "            description=\"description of the entry type\",\n",
        "            type_aliases=typeAliases,\n",
        "            required_aspects=[],\n",
        "        )\n",
        "        create_operation = client.create_entry_type(\n",
        "            parent=parent, entry_type=entry_type, entry_type_id=entry_type_id\n",
        "        )\n",
        "        return create_operation.result(60)"
      ],
      "metadata": {
        "id": "Sfu16BYrsDBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates Dataplex Aspect Types\n",
        "\n",
        "def create_aspect_type(\n",
        "    project_id: str, location: str, aspect_type_id: str,\n",
        ") -> dataplex_v1.AspectType:\n",
        "    \"\"\"Create Aspect Type identified by aspect_type_id, located in project_id, location \"\"\"\n",
        "\n",
        "    aspect_fields = List[dataplex_v1.AspectType.MetadataTemplate]\n",
        "\n",
        "    with dataplex_v1.CatalogServiceClient() as client:\n",
        "        # The resource name of the Aspect Type location\n",
        "        parent = f\"projects/{project_id}/locations/{location}\"\n",
        "\n",
        "        # Define the Aspect Type resource.\n",
        "        # It requires a metadata_template (a JSON schema) to define the\n",
        "        # properties of the Aspect.\n",
        "\n",
        "        aspect_field = dataplex_v1.AspectType.MetadataTemplate(\n",
        "        name=\"name_of_the_field\",\n",
        "        # Metadata Template is recursive structure,\n",
        "        # primitive types such as \"string\" or \"integer\" indicate leaf node,\n",
        "        # complex types such as \"record\" or \"array\" would require nested Metadata Template\n",
        "        type=\"string\",\n",
        "        index=1,\n",
        "        annotations=dataplex_v1.AspectType.MetadataTemplate.Annotations(\n",
        "            description=\"description of the field\"\n",
        "        ),\n",
        "        constraints=dataplex_v1.AspectType.MetadataTemplate.Constraints(\n",
        "            # Specifies if field will be required in Aspect Type.\n",
        "            required=False\n",
        "        ),\n",
        "        )\n",
        "\n",
        "        aspect_fields = [aspect_field]\n",
        "\n",
        "        aspect_type = dataplex_v1.AspectType(\n",
        "            description=f\"description of aspect type {aspect_type_id}\",\n",
        "            metadata_template=dataplex_v1.AspectType.MetadataTemplate(\n",
        "                name=\"name_of_the_template\",\n",
        "                type=\"record\",\n",
        "                # Aspect Type fields, that themselves are Metadata Templates.\n",
        "                record_fields=aspect_fields,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        create_operation = client.create_aspect_type(\n",
        "            parent=parent,\n",
        "            aspect_type=aspect_type,\n",
        "            aspect_type_id=aspect_type_id\n",
        "        )\n",
        "\n",
        "        # Dataplex operations are long-running, so we wait for the result.\n",
        "        # The timeout is set to 120 seconds (2 minutes).\n",
        "        print(\"Waiting for creation operation to complete...\")\n",
        "        try:\n",
        "            response = create_operation.result(timeout=30)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during Aspect Type creation: {e}\")\n",
        "            raise\n",
        "\n",
        "        print(\n",
        "            f\"Successfully created Aspect Type: projects/{project_id}/locations/{location}/aspectTypes/{aspect_type_id}\"\n",
        "        )\n",
        "        return response"
      ],
      "metadata": {
        "id": "D4gMNgxgsJTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create required entry types, aspect types, and entry group for the metadata import file\n",
        "\n",
        "# Oracle dataplex metadata types found in the file\n",
        "entry_types = ['oracle-instace','oracle-database','oracle-schema','oracle-table','oracle-view']\n",
        "aspect_types = ['oracle-instance','oracle-database','oracle-schema','oracle-table','oracle-view']\n",
        "entry_group_id = \"oracle\"\n",
        "\n",
        "successCount = 0\n",
        "\n",
        "# create the entry group\n",
        "try:\n",
        "  create_entry_group(project_id, dataplex_location, entry_group_id)\n",
        "  successCount += 1\n",
        "except Exception as e:\n",
        "  print(f\"Exception creating entry group {entry_group_id}: {e}\")\n",
        "\n",
        "# Create entry types\n",
        "for et in entry_types:\n",
        "  try:\n",
        "    create_entry_type(project_id, dataplex_location,et)\n",
        "    successCount += 1\n",
        "  except Exception as e:\n",
        "    print(f\"Exception creating entry type {et}: {e}\")\n",
        "\n",
        "# Create aspect types\n",
        "for at in aspect_types:\n",
        "  try:\n",
        "    create_aspect_type(project_id, \"global\",at)\n",
        "    successCount += 1\n",
        "  except Exception as e:\n",
        "    print(f\"Exception creating aspect type {at}: {e}\")\n",
        "\n",
        "if successCount == len(entry_types) + len(aspect_types) + 1:\n",
        "  print(\"All dataplex metadata hierarchy types created successfully\")"
      ],
      "metadata": {
        "id": "6o8i9kAWsSsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH10X1BPNhb2"
      },
      "source": [
        "### <font color='#4285f4'>Import the metadata Import file into Dataplex Universal Catalog</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5mcBIKjNhb3"
      },
      "source": [
        "https://cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.metadataJobs/create\n",
        "\n",
        "Replace the following\n",
        "- project_id         \"GCP project id\"\n",
        "- dataplex_location   \"target location for metadata in the dataplex catalog. ie a GCP region or \"global\"\"\n",
        "\n",
        "```\n",
        "curl -X POST -H 'Content-Type: application/json; charset=utf-8' -H \"Authorization: Bearer $(gcloud -q auth print-access-token)\" \\\n",
        "-d @metadata_import_request.json \\\n",
        "https://dataplex.googleapis.com/v1/projects/${project_id}/locations/${dataplex_location}/metadataJobs?metadataJobId=a001\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_request_file_url = f\"gs://{governed_data_code_bucket}/oracle_exports/metadata_import_request.json\"\n",
        "print(f\"Downloading the metadata import request file from {json_request_file_url}\")\n",
        "\n",
        "!gsutil cp \"$json_request_file_url\" ."
      ],
      "metadata": {
        "id": "VSPpKsJt0NhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color='#4285f4'>Submit import job</font>"
      ],
      "metadata": {
        "id": "ty9tg2aiUQs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initiate a metadata import job\n",
        "\n",
        "submission_success = False\n",
        "\n",
        "# Build unique bath ID for the import job\n",
        "now = datetime.datetime.now()\n",
        "IMPORT_BATCH_ID = f\"metadata{now.day}{now.hour}{now.minute}{now.microsecond}\"\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "# creds.valid is False, and creds.token is None\n",
        "# Need to refresh credentials to populate\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "# Now can use creds.token\n",
        "\n",
        "json_request = loadReferencedFile(\"metadata_import_request.json\")\n",
        "\n",
        "endpoint = f\"https://dataplex.googleapis.com/v1/projects/{project_id}/locations/global/metadataJobs?metadataJobId={IMPORT_BATCH_ID}\"\n",
        "requestbody = json.loads(json_request)\n",
        "headers = {\"Authorization\": f\"Bearer {creds.token}\", \"Content-Type\": \"application/json; charset=utf-8\"}\n",
        "\n",
        "JOB_START_TIME = datetime.datetime.now()\n",
        "\n",
        "# Submit metadataimport job\n",
        "try:\n",
        "  responseRaw = requests.post(endpoint, data=json_request, headers=headers)\n",
        "  responseJSON = responseRaw.json()\n",
        "  if 'error' in responseJSON:\n",
        "    print (responseJSON['error'])\n",
        "  else:\n",
        "    submission_success = True\n",
        "\n",
        "    print(f\"Metata import job {IMPORT_BATCH_ID} submitted successfully\")\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"Exception calling metadata import job: {e}\")"
      ],
      "metadata": {
        "id": "U48b_uww6tzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color='#4285f4'>Track import status</font>"
      ],
      "metadata": {
        "id": "ipbdOlFcUZJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Poll the import endpoint to check status of import job progress\n",
        "# The import takes around 16 minutes\n",
        "\n",
        "if submission_success:\n",
        "  endpoint = f\"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}/locations/global/metadataJobs/{IMPORT_BATCH_ID}\"\n",
        "  headers = {\"Authorization\": f\"Bearer {creds.token}\"}\n",
        "\n",
        "  # Loop until job complete\n",
        "  while True:\n",
        "    print(f\"Checking job progress for metadata import job\")\n",
        "    responseRaw2 = requests.get(endpoint, headers=headers)\n",
        "    responseJSON2 = responseRaw2.json()\n",
        "    now = datetime.datetime.now()\n",
        "    timeDiff = now - JOB_START_TIME\n",
        "\n",
        "    if 'status' in responseJSON2:\n",
        "      print(f\"Job ID: {IMPORT_BATCH_ID}   Job status: {responseJSON2['status']['state']}    Running time so far: {timeDiff}\")\n",
        "      if (responseJSON2['status']['state'] not in ['QUEUED','RUNNING']):\n",
        "        print(responseJSON2)\n",
        "        print(\"JOB completed\")\n",
        "        break\n",
        "      else:\n",
        "        time.sleep(30)\n",
        "    else:\n",
        "      print(responseJSON2)"
      ],
      "metadata": {
        "id": "ysVUL-eY5n86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASQ2BPisXDA0"
      },
      "source": [
        "### <font color='#4285f4'>Reference Links</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTY6xJdZ3ul8"
      },
      "source": [
        "- [Dataplex Managed Connectivity Overview](https://cloud.google.com/dataplex/docs/import-metadata)\n",
        "- [Import customer metadata](https://cloud.google.com/dataplex/docs/import-metadata)\n",
        "- [Community Contributed Connectors](https://cloud.google.com/dataplex/docs/managed-connectivity-overview#community-contributed-connectors)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "U7Wx2xZrPfN2",
        "i-D4JI3QPfN3",
        "HMsUvoF4BP7Y",
        "m65vp54BUFRi",
        "UmyL-Rg4Dr_f",
        "c51M89g0Ejmz"
      ],
      "name": "10-Dataplex-Universal-Catalog-External-Data.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}